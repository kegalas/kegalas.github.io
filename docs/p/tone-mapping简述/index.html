<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='如何显示HDR （注：本人对显示器技术了解有限，这一部分可能不够准确。另外，国内外网络上关于这些东西的说明实在太少，结合了我的一些个人理解，我'><title>Tone Mapping简述</title>

<link rel='canonical' href='https://kegalas.uk/p/tone-mapping%E7%AE%80%E8%BF%B0/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='Tone Mapping简述'>
<meta property='og:description' content='如何显示HDR （注：本人对显示器技术了解有限，这一部分可能不够准确。另外，国内外网络上关于这些东西的说明实在太少，结合了我的一些个人理解，我'>
<meta property='og:url' content='https://kegalas.uk/p/tone-mapping%E7%AE%80%E8%BF%B0/'>
<meta property='og:site_name' content='KegalaS的个人博客'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='大学' /><meta property='article:tag' content='人工智能' /><meta property='article:tag' content='计算机视觉' /><meta property='article:tag' content='图形学' /><meta property='article:published_time' content='2025-02-15T14:43:24&#43;08:00'/><meta property='article:modified_time' content='2025-02-15T14:43:24&#43;08:00'/><meta property='og:image' content='https://kegalas.uk/p/tone-mapping%E7%AE%80%E8%BF%B0/cover.jpg' />
<meta name="twitter:title" content="Tone Mapping简述">
<meta name="twitter:description" content="如何显示HDR （注：本人对显示器技术了解有限，这一部分可能不够准确。另外，国内外网络上关于这些东西的说明实在太少，结合了我的一些个人理解，我"><meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content='https://kegalas.uk/p/tone-mapping%E7%AE%80%E8%BF%B0/cover.jpg' />
    <link rel="shortcut icon" href="/images/favicon.ico" />

    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>返回</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="has-image main-article">
    <header class="article-header">
        <div class="article-image">
            <a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/">
                <img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/cover_hue2557a0ca715ca3bab4b5f5b903f073d_123247_800x0_resize_q75_box.jpg"
                        srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/cover_hue2557a0ca715ca3bab4b5f5b903f073d_123247_800x0_resize_q75_box.jpg 800w, /p/tone-mapping%E7%AE%80%E8%BF%B0/cover_hue2557a0ca715ca3bab4b5f5b903f073d_123247_1600x0_resize_q75_box.jpg 1600w"
                        width="800" 
                        height="307" 
                        loading="lazy"
                        alt="Featured image of post Tone Mapping简述" />
                
            </a>
        </div>
    

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/" >
                图形学
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/">Tone Mapping简述</a>
    </h2>

    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Feb 15, 2025</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 19 分钟
                </time>
            </div>
        
    </footer>
    
</div>
</header>

    <section class="article-content">
    <h1 id="如何显示hdr">如何显示HDR</h1>
<p>（注：本人对显示器技术了解有限，这一部分可能不够准确。另外，国内外网络上关于这些东西的说明实在太少，结合了我的一些个人理解，我尽量做到不自我矛盾）</p>
<p>白色<span class="math inline">\([255,255,255]\)</span>、红色<span class="math inline">\([255,0,0]\)</span>这样的颜色是怎么显示在屏幕上的呢？首先，将其转化为标准化的<span class="math inline">\([1.0, 1.0, 1.0]\)</span>、<span class="math inline">\([1.0, 0.0, 0.0]\)</span>，然后发送到显示器，显示器中分别显示RGB三种颜色的灯泡就会按照这个值去发出特定的亮度，这个特定的亮度由电光转换（EOTF）函数决定，在SDR显示器中，这个函数通常是gamma传递函数</p>
<p><span class="math display">\[Y = E^\gamma
\]</span></p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 108; 
			flex-basis: 259px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/1.jpg" data-size="365x337">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/1.jpg"
			width="365"
			height="337"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/1_hu4d988d9e5993e7af8bb8d46a2379b0c6_21162_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/1_hu4d988d9e5993e7af8bb8d46a2379b0c6_21162_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="1.jpg">
	</a>
	
	<figcaption>1.jpg</figcaption>
	
</figure></p>
<p>这里<span class="math inline">\(E\)</span>是我们给予的颜色值<span class="math inline">\([0, 1]\)</span>，而<span class="math inline">\(Y\)</span>是标准化的亮度值<span class="math inline">\([0,1]\)</span>，其中<span class="math inline">\(0\)</span>代表灯泡的最低亮度，而<span class="math inline">\(1\)</span>代表灯泡的最高亮度。<span class="math inline">\(\gamma\)</span>就是我们说的gamma值，一般会取<span class="math inline">\(2.2\)</span>。一般来说，SDR显示器的最高亮度定义为<span class="math inline">\(100\)</span>nits，所以</p>
<p><span class="math display">\[F_D = 100Y = 100E^\gamma
\]</span></p>
<p>其中<span class="math inline">\(F_D\)</span>是显示器理论上会发出的亮度值。</p>
<p>HDR的含义是高动态范围，要求我们能够显示更高的亮度，即超越<span class="math inline">\(100\)</span>nits达到<span class="math inline">\(400\)</span>、<span class="math inline">\(1000\)</span>甚至更高的nits。虽然理论上来说你确实可以将<span class="math inline">\(F_D = 400Y\)</span>来适应更高的亮度，但是使用其他曲线能够更好的符合人眼感知特性（以及其他原因，见后）。特别的，在HDR领域用的最多的是PQ曲线和HLG曲线，这里我们主要讲PQ：</p>
<p><span class="math display">\[F_D = 10000\left(\dfrac{\max[(E^{1/m_2}-c_1),0]}{c_2-c_3\cdot E^{1/m_2}}\right)^{1/m_1}
\]</span></p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 112; 
			flex-basis: 270px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/2.png" data-size="664x590">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/2.png"
			width="664"
			height="590"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/2_hucb57838621395c217f2c74f1db6ee6d9_41157_480x0_resize_box_3.png 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/2_hucb57838621395c217f2c74f1db6ee6d9_41157_1024x0_resize_box_3.png 1024w"
			loading="lazy"
			alt="2.jpg">
	</a>
	
	<figcaption>2.jpg</figcaption>
	
</figure></p>
<p>具体的参数数值可以在<a class="link" href="https://en.wikipedia.org/wiki/Perceptual_quantizer"  target="_blank" rel="noopener"
    >https://en.wikipedia.org/wiki/Perceptual_quantizer</a>找到。由公式和图像可知，PQ曲线最高支持到10000nits亮度。</p>
<p>PQ曲线是同样的，那对于只有400nits亮度的显示屏如何显示1.0大小的信号呢？答案是：不要使用1.0大小的信号。通过某一些方法（见后，即tonemapping）将1.0大小的信号缩放到0.7左右，并且视频、图像文件里都只存0.7以下的数字。</p>
<p>实际上这并不是乱说的，HDR视频制作过程中metadata确实会有记录视频制作时的显示器亮度，以及视频播放时显示器的亮度（至少HDR Vivid确实有），这对于HDR的合理播放至关重要。</p>
<p>当然，这是假设了一个显示器直接根据PQ值输出亮度，实际上有些显示器自带tone mapping将PQ曲线规定的0~10000nits映射到自己的亮度值如0~400nits。据说也有的显示器是直接将超出的亮度裁剪掉。</p>
<p>接下来我们再来说说位深。从刚刚的描述中你也注意到，我们完全可以用<span class="math inline">\([0,255]\)</span>来表示HDR的所有亮度值<span class="math inline">\([0,1]\)</span>啊，为什么我们经常说HDR必须用上<span class="math inline">\(10\)</span>bit呢？</p>
<p>我们先以简单的线性视角来看，如果要量化<span class="math inline">\([0,100]\)</span>nits这个区间，我们使用<span class="math inline">\(8\)</span>位深度，那么就是用<span class="math inline">\(256\)</span>个数去表示<span class="math inline">\([0,100]\)</span>，每两个连续的量化之间的差异是<span class="math inline">\(100/256\approx 0.4\)</span>。而如果我们要量化<span class="math inline">\([0,10000]\)</span>nits这个区间，那么差异就是<span class="math inline">\(10000/256\approx40\)</span>，大了一百倍，这造成的结果就是渐变变化跨度太大，有“条带”现象的出现。举个例子如下，从左到右量化精度越来越高，两个连续量化之间的差异越来越小。</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 163; 
			flex-basis: 392px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/3.jpg" data-size="1015x621">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/3.jpg"
			width="1015"
			height="621"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/3_hua217a8ec1bbbbf5071b4d26b53ab773e_33672_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/3_hua217a8ec1bbbbf5071b4d26b53ab773e_33672_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="3.jpg">
	</a>
	
	<figcaption>3.jpg</figcaption>
	
</figure></p>
<p>问题是10bit用来量化10000nits他仍然有<span class="math inline">\(10000/1024\approx 10\)</span>的连续差异啊？为什么HDR显示器10bit甚至8抖10bit就可以满足要求？这时我们就要脱离出线性的视角，实际上，在PQ域里，100nits对应的信号值是<span class="math inline">\(0.5081\)</span>，而10000nits对应的是<span class="math inline">\(1.0\)</span>。这就意味着我们使用10bit位深，来量化<span class="math inline">\([0,1.0]\)</span>这个区间，虽然<span class="math inline">\([0,511]\)</span>负责量化<span class="math inline">\([0,0.5]\)</span>，而<span class="math inline">\([512, 1023]\)</span>负责量化<span class="math inline">\([0.5, 1.0]\)</span>，但是将其使用PQ曲线转化后，实际上<span class="math inline">\([0, 511]\)</span>量化了<span class="math inline">\([0, 100]\)</span>nits，而<span class="math inline">\([512, 1023]\)</span>量化了<span class="math inline">\([100, 10000]\)</span>nits。换句话说，亮度越低的地方量化误差越小。</p>
<p>根据<a class="link" href="https://en.wikipedia.org/wiki/High-dynamic-range_television#Transfer_function"  target="_blank" rel="noopener"
    >https://en.wikipedia.org/wiki/High-dynamic-range_television#Transfer_function</a>所说，想要避免条带问题，在PQ域量化完整的10000nits需要至少12bits。而我们一般的HDR显示器也就支持到400nits，好一点的1000nits，用10bit来量化这些亮度低的地方完全足够。</p>
<p>回到之前的问题，为什么不再用gamma曲线了呢？除了人眼感知的一些因素，还因为gamma域中的10000nits量化要避免条带问题，至少需要15bit的位深。所以说白了，其实传递函数是对于量化亮度的分配规则（虽然gamma最初来源于CRT显示器的物理特性），将人眼最敏感的那部分区域用最多的空间去表示，而其他不敏感的地方用较少的空间区表示。</p>
<p>接下来还有颜色空间的问题。HDR10这样的标准确实规定了颜色空间必须是Rec2020，但是从上面的介绍中你也认识到，要显示HDR和颜色空间没有什么关系，你就算只支持灰度显示也可以HDR。不过，你若想将HDR映射到SDR显示器上显示，你仍然需要在tonemapping过后将Rec2020转到Rec709，防止SDR显示器显示有偏差的颜色。</p>
<h1 id="tone-mapping的作用">Tone Mapping的作用</h1>
<p>最初是没有HDR显示器的，Tone mapping的作用是将用各种技术捕捉到的具有高亮度的图像，恰当的显示在SDR显示器上。通常来说，一张照片中的很大部分内容都在<span class="math inline">\([0,100]\)</span>nits之间，可以被SDR显示器正常显示，而只有很少一部分（例如太阳直射），具有很高的亮度，比如1000nits。而Tone mapping要做的事是，保留大部分正常显示的内容亮度不降低太少，而大幅度降低过于亮的部分。例如，我们可以用<span class="math inline">\([0,80]\)</span>nits来显示原来<span class="math inline">\([0,100]\)</span>的内容，而使用<span class="math inline">\([80, 100]\)</span>nits显示所有大于100nits的内容。实际上，HDR显示器的最高亮度也各不相通，如何将适配于HDR1000的视频映射到HDR400也是Tone Mapping需要做的事。</p>
<p>你可能会说这和刚刚提到的EOTF很像，都是在限定的范围内合理分配数值。那我们可不可以直接将PQ域的数据转换到线性域，再从线性域转换到Gamma域呢？答案是可以，但是不够好。借用论文<a class="link" href="https://www.sciencedirect.com/science/article/pii/S1051200423001100"  target="_blank" rel="noopener"
    >High Dynamic Range Image Tone Mapping: Literature review and performance benchmark</a>中的一张图（本文之后的内容也会极大程度参考该综述）</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 170; 
			flex-basis: 408px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/4.jpg" data-size="1213x712">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/4.jpg"
			width="1213"
			height="712"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/4_hud25faf95cd4b7588858ba3015aed4756_136735_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/4_hud25faf95cd4b7588858ba3015aed4756_136735_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="4.jpg">
	</a>
	
	<figcaption>4.jpg</figcaption>
	
</figure></p>
<p>使用不同的Tone mapping效果是不一样的。上面的gamma曲线的图像明显是不如Drago曲线的，也就是说，gamma曲线对于HDR源的压缩还不够好，因为它毕竟只是为了SDR源而设计的。</p>
<p>在不同地方，Tone mapping曲线的目标可能是不一样的。例如在图形渲染当中，为了符合更好的物理规律，镜面反射+漫反射+环境光的亮度可能会大于1.0，这里的Tone mapping目的在于将<span class="math inline">\([0, +\infty]\)</span>的亮度映射到<span class="math inline">\([0, 1]\)</span>之间，然后再去gamma校正。而对于HDR Vivid这样的视频处理，其规则为：PQ域-线性域-Tone mapping-Gamma域，首先转化为<span class="math inline">\([0,1]\)</span>之间的标准化亮度，然后Tone mapping负责将这个<span class="math inline">\([0,1]\)</span>映射到另一个<span class="math inline">\([0,1]\)</span>上，更符合Gamma域的颜色分布，然后进行伽马校正。</p>
<p>通常，Tone mapping是在“亮度”上进行的，而非分别在RGB上做（当然也有），在改变亮度后再进行“颜色恢复”，形式如下</p>
<p><span class="math display">\[C' = \left(\dfrac{C}{L}\right)^s T
\]</span></p>
<p>其中，<span class="math inline">\(C\)</span>是指原图中某个像素的RGB通道中的任意一个的数值，而<span class="math inline">\(C'\)</span>则是其对应的Tone mapping后的结果数值。<span class="math inline">\(L\)</span>是该像素的亮度，而<span class="math inline">\(T\)</span>是<span class="math inline">\(L\)</span>使用Tone mapping曲线计算出来的结果，<span class="math inline">\(s\)</span>是颜色补偿参数，一般会取<span class="math inline">\(1\)</span>，如果小于<span class="math inline">\(1\)</span>就会欠饱和。所以，面向亮度进行的Tone mapping就是在研究如何将<span class="math inline">\(L\)</span>转换成<span class="math inline">\(T\)</span>。</p>
<p>首先先谈一谈如何计算某个像素的亮度值。这通常很经验，要么就是根据某种人眼的生物学研究，但是有几个形式用的比较多，最多的可能是：</p>
<p><span class="math display">\[L = 0.2959R + 0.5870G + 0.1140B
\]</span></p>
<p>即RGB的加权平均（事实上，人眼确实对绿色的亮度最敏感）。而像HDR Vivid这样的体系会使用<span class="math inline">\(L=\max\{R,G,B\}\)</span>。虽然没有确切证据证据，但是前者恰好是RGB转YUV中Y的定义，而后者正好是RGB转HSV中V的定义。</p>
<h1 id="只考虑单个像素亮度的tmo">只考虑单个像素亮度的TMO</h1>
<p>（注：TMO是Tone Mapping Operator）</p>
<p>如果我们整张图只用一根曲线，并且只考虑某个像素自己的信息，那么这也可以叫做Global TMO。</p>
<p>例如，<strong>极简化</strong>的Reinhard方法可以被写作</p>
<p><span class="math display">\[T = \dfrac{L}{1+L}
\]</span></p>
<p>其将<span class="math inline">\([0,+\infty]\)</span>映射到<span class="math inline">\([0, 1]\)</span>上。更多的这种用在游戏领域的GTMO可以看<a class="link" href="https://zhuanlan.zhihu.com/p/21983679"  target="_blank" rel="noopener"
    >https://zhuanlan.zhihu.com/p/21983679</a>，它们的显著特点就是形式简单，速度超快。（另外他这里面全是RGB上直接做的，而Reinhard的<a class="link" href="https://dl.acm.org/doi/abs/10.1145/3596711.3596781"  target="_blank" rel="noopener"
    >原论文</a>取的是<span class="math inline">\(L=0.27R+0.67G+0.06B\)</span>）</p>
<p>这样的单根曲线很多时候是根据人眼视觉系统（human visual system, HVS）的特性来设计的，例如论文<a class="link" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.00689"  target="_blank" rel="noopener"
    >Adaptive logarithmic mapping for displaying high contrast scenes</a>中，就使用了</p>
<p><span class="math display">\[L_d = \dfrac{L_{dmax}\cdot 0.01}{\log_{10}(L_{wmax}+1)}\cdot\dfrac{\log(L_w+1)}{\log\left(2+\left(\left(\dfrac{L_w}{L_{wmax}}\right)^{\dfrac{\log(b)}{\log(0.5)}}\right)\cdot 8\right)}
\]</span></p>
<p>这样的公式，这里的<span class="math inline">\(L_w\)</span>就是映射前的亮度，<span class="math inline">\(L_d\)</span>就是映射后的亮度，而<span class="math inline">\(L_{dmax}\)</span>一般取100代表显示器的最大亮度，<span class="math inline">\(b\)</span>是可调参数。</p>
<p>还有一种方法是根据直方图来的，例如<a class="link" href="https://www.sciencedirect.com/science/article/pii/S0031320309004518"  target="_blank" rel="noopener"
    >Tone-mapping High Dynamic Range Images by Novel Histogram Adjustment</a>可以被<strong>极简化</strong>的写作</p>
<p><span class="math display">\[D(I) = (D_{max}-D_{min})\times \dfrac{\log(I+\tau)-\log(I_{min}+\tau)}{\log(I_{max}+\tau)-\log(I_{min}+\tau)}+D_{min}
\]</span></p>
<p>其中<span class="math inline">\(D_{max}, D_{min}\)</span>是显示器的最大最小亮度，而<span class="math inline">\(I_{max}, I_{min}\)</span>是场景中的最大最小亮度。（注：曲线对于同一张图片是固定的，不同位置的像素拥有相同的输入就有相同的输出）</p>
<p>也有研究者引入了聚类算法，对不同的类别进行统计，决定该图像的映射曲线使用什么参数。</p>
<p>这类GTMO的问题在于，其对于局部对比度的保护不好。尤其是在特别亮和特别暗的地方，这些曲线倾向于将它们映射到非常接近的亮度，而一个像素和其周围的像素亮度通常是接近的，使用GTMO就丢失了局部对比度。</p>
<h1 id="考虑周围像素亮度值的tmo">考虑周围像素亮度值的TMO</h1>
<p>为了解决局部对比度的问题，Local TMO就被提出了。GTMO理论上同一张图片里相同的亮度会被映射成相同的结果，但LTMO就允许映射成不同的结果，来保护局部的对比度。</p>
<p>考虑空间上的信息的研究中比较著名的是<strong>完全体</strong>版<a class="link" href="https://dl.acm.org/doi/abs/10.1145/3596711.3596781"  target="_blank" rel="noopener"
    >Reinhard</a>，在传统的摄影技法中，摄影师能够通过调整曝光时间来将结果中的暗的地方增亮，亮的地方变暗，从而让结果包含更多有效信息。Reinhard方法通过自动的方法来进行这种操作（实际上是反向操作）。作者使用高斯滤波器来找到这种局部亮度差不多的像素，然后通过减被亮像素包围的暗像素，提亮被暗像素包围的亮像素，于是就成功的将局部的对比度提高了。</p>
<p><span class="math display">\[V(x,y,s) = \dfrac{V_1(x,y,s)-V_2(x,y,s)}{2^\phi a/s^2+V_1(x,y,s)}
\]</span></p>
<p>这里的公式里的<span class="math inline">\(V_i\)</span>就是高斯滤波后的结果，其中<span class="math inline">\(s\)</span>控制了方差大小，<span class="math inline">\(a,\phi\)</span>是可调参数。<span class="math inline">\(V_1, V_2\)</span>的区别在于其方差被乘以了一个不同的常数，<span class="math inline">\(V_1\)</span>乘以了比较小的数，而<span class="math inline">\(V_2\)</span>乘以了比较大的数。</p>
<p>在图片中<span class="math inline">\(V_1\)</span>和<span class="math inline">\(V_2\)</span>在亮度梯度小的地方数值差距小，在亮度梯度大，也就是局部对比度大的地方数值差距大。作者通过搜索<span class="math inline">\(s\)</span>的方法，来让这整个式子的绝对值第一次小于一个限定值。通过这样的方式就可以寻找到一个local区域，其用<span class="math inline">\(s_m\)</span>来表示这个区域的大小。</p>
<p>分母上的<span class="math inline">\(V_1\)</span>来让这个式子和亮度绝对值无关，而<span class="math inline">\(2^\phi a/s^2\)</span>则可以防止<span class="math inline">\(V\)</span>变成极端值。计算完最优的<span class="math inline">\(s_m\)</span>后，这里的<span class="math inline">\(V_1\)</span>就可以用来表示该像素的局部平均值，于是我们就可以修改之前的极简版Reinhard为：</p>
<p><span class="math display">\[L_d(x,y) = \dfrac{L(x,y)}{1+V_1(x,y,s_m(x,y))}
\]</span></p>
<p>当遇到亮区域的暗像素时，就有<span class="math inline">\(L< V_1\)</span>，于是<span class="math inline">\(L_d\)</span>就会更小，即让差不多亮的地方的暗的地方更暗一点，反之则让亮的地方更亮一点，从而增加了局部的对比度。</p>
<p>而在频域的信息上来做的最著名的是<a class="link" href="https://dl.acm.org/doi/abs/10.1145/566570.566574"  target="_blank" rel="noopener"
    >Durand &amp; Dorsey</a>方法，原文大篇幅在讲滤波，我看不懂，但是其LTMO的思路如下（参考自<a class="link" href="https://zhuanlan.zhihu.com/p/573894977"  target="_blank" rel="noopener"
    >https://zhuanlan.zhihu.com/p/573894977</a>）</p>
<ol>
<li>通过我们之前加权求和方法计算图片的亮度图，并且将其进行对数运算，得到<strong>对数域</strong>亮度图<span class="math inline">\(L\)</span></li>
<li>在<span class="math inline">\(L\)</span>上进行双边滤波，得到<span class="math inline">\(B\)</span>，即代表了基础层，我们需要压缩这部分</li>
<li>计算细节信息层（局部对比度就在这里）<span class="math inline">\(D=L-B\)</span></li>
<li>以某种方法压缩<span class="math inline">\(B\)</span>，例如直接乘以常数，或者使用某种GTMO（我试过真的可以，不过要先从对数域转换回去，之后再转换回来），得到压缩后的结果<span class="math inline">\(B'\)</span></li>
<li>重新算出全图亮度<span class="math inline">\(L'=B'+D+\beta\)</span>，这里的<span class="math inline">\(\beta\)</span>是曝光补偿参数，可调。</li>
<li>将<span class="math inline">\(L'\)</span>从对数域转回去，得到了新的亮度，然后使用我们之前说的颜色恢复方法恢复回去就可以了。</li>
</ol>
<p>当然，这里的细节层也不是不可以增强或者压缩，看自己的选择。这种方法的原理是利用了双边滤波的特性，其可以保护边缘信息，而将其他变化平缓的部分取均值，从而我们可以通过计算出基础层<span class="math inline">\(B\)</span>，然后用用<span class="math inline">\(L-B\)</span>得到细节层<span class="math inline">\(D\)</span>。实际上这个<span class="math inline">\(D\)</span>就是将原图中的边缘信息去除，然后留下平缓部分的“差异”，也就是“局部对比度”。</p>
<p>之前用双边滤波可能会有点性能问题，但是双边网格被发明后，再加上现在电脑的算力提升，这种方法也是逐渐可以用到游戏里了，例如<a class="link" href="https://advances.realtimerendering.com/s2021/jpatry_advances2021/index.html#/125"  target="_blank" rel="noopener"
    >对马岛之魂</a>中，就应用了这种技术，其算法我认为本质上和上文的是相同的：</p>
<p><span class="math display">\[I_o=c\times (B-M) + d\times (I_i-B) + M
\]</span></p>
<p>上面这些全在<strong>对数域</strong>，<span class="math inline">\(I_o, I_i\)</span>就是输出和输入亮度，<span class="math inline">\(B\)</span>是滤波得到的基础层，<span class="math inline">\(M\)</span>是亮度的中值。<span class="math inline">\(c\)</span>是调整对比度的可调参数，<span class="math inline">\(d\)</span>是调整细节的参数。</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 168; 
			flex-basis: 405px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/6.jpg" data-size="574x340">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/6.jpg"
			width="574"
			height="340"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/6_huf5f553ec2db9ab23c0d1891afbb9f3a2_13554_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/6_huf5f553ec2db9ab23c0d1891afbb9f3a2_13554_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="6.jpg">
	</a>
	
	<figcaption>6.jpg</figcaption>
	
</figure></p>
<p>双边滤波的问题是会有光晕问题。作者给出的解释如下</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 214; 
			flex-basis: 515px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/7.jpg" data-size="1286x599">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/7.jpg"
			width="1286"
			height="599"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/7_hu70f0d20628d8dbcf7d8f34ff26d89e4f_101894_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/7_hu70f0d20628d8dbcf7d8f34ff26d89e4f_101894_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="7.jpg">
	</a>
	
	<figcaption>7.jpg</figcaption>
	
</figure></p>
<p>我认为作者的意思是想说，在减少对比度的同时，增加细节，就会让这根曲线从“大体上单增”，变成了一个“大体上先增后减再增“的曲线，正是这种奇怪的过渡使得光晕产生。或者用“梯度反转”这个词来表示，也就是说，原来亮度高的地方变成了亮度低的，而亮度低的变得亮度高了，体现出来就是围绕着边缘处有一圈比边缘内更亮的区域，即“光晕”。</p>
<p>而作者给出的解决办法就是混合使用高斯滤波和双边滤波。大概40%的双边，60%的高斯，而且高斯核必须足够大。</p>
<h1 id="考虑像素颜色值的tmo">考虑像素颜色值的TMO</h1>
<p>我们之前说的颜色恢复并不是那么准确，像那样改亮度实际上是会稍微影响到色度的。</p>
<p>TODO</p>
<h1 id="深度学习的tmo">深度学习的TMO</h1>
<h2 id="基于gan的监督学习方法">基于GAN的监督学习方法</h2>
<p>例如论文<a class="link" href="https://ieeexplore.ieee.org/abstract/document/8822603/"  target="_blank" rel="noopener"
    >Deep tone mapping operator for high dynamic range images</a>提出的DeepTMO，其使用cGAN的架构来对给出的HDR图片进行LDR图片的生成</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 195; 
			flex-basis: 469px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/8.jpg" data-size="703x359">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/8.jpg"
			width="703"
			height="359"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/8_hu0ce6287c1e158247b04fe1858dfb0b61_35413_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/8_hu0ce6287c1e158247b04fe1858dfb0b61_35413_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="8.jpg">
	</a>
	
	<figcaption>8.jpg</figcaption>
	
</figure></p>
<p>如图，其从训练数据中拿出HDR和其对应的真实LDR图片的亮度图给判别器，让其判断LDR是否是从HDR中生成的。而生成器的任务就是从HDR亮度图中生成假的LDR亮度图，然后尽可能让判别器判断不出来。</p>
<p>训练数据是作者实现了一系列经典的TMO，然后用他们将HDR图片数据集转化为LDR图片，使用客观评价标准Tone Mapped Image Quality Index (TMQI)来判断表现最好的LDR，然后将其作为真实LDR标注图片。</p>
<p>本文的想法是之前的TMO对不同场景的表现不同，在高亮度表现好的TMO不一定在低亮度表现好，或者这些TMO需要调整参数才能适应不同的场景。本文提出的DeepTMO希望免去输入参数的步骤，直接进行场景的自适应，对不同条件的光照都生成最好的结果。</p>
<p>其单尺度的网络结构如下</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 407; 
			flex-basis: 978px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/9.jpg" data-size="1382x339">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/9.jpg"
			width="1382"
			height="339"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/9_hue1ca29aea814dfa0d175f2808d501eea_66466_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/9_hue1ca29aea814dfa0d175f2808d501eea_66466_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="9.jpg">
	</a>
	
	<figcaption>9.jpg</figcaption>
	
</figure></p>
<p>生成器是一个卷积、残差块、反卷积的编解码器结构，输入HDR亮度图输出同分辨率的LDR亮度图。而判别器的架构是PatchGAN，每次只对一个<span class="math inline">\(70\times 70\)</span>的空间进行判断，最终再将所有Patch的判断综合起来进行全图的判断。用PatchGAN的好处，作者认为是参数少，并且可以应用到任意大小的图片上。</p>
<p>然而单尺度的DeepTMO还是太弱了，有细节处的伪影，所以作者又提出了多尺度的加强版。</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 187; 
			flex-basis: 450px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/10.jpg" data-size="662x353">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/10.jpg"
			width="662"
			height="353"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/10_hu3dd5ffd10a3fc72079ca062782f1c0c3_43723_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/10_hu3dd5ffd10a3fc72079ca062782f1c0c3_43723_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="10.jpg">
	</a>
	
	<figcaption>10.jpg</figcaption>
	
</figure></p>
<p>作者这里只给出了生成器的结构。上图所示的两个生成器<span class="math inline">\(G_d,G_o\)</span>都和原来的架构是一样的，<span class="math inline">\(G_d\)</span>接收下采样过两次的输入。<span class="math inline">\(G_o\)</span>负责进行局部细节的Tone mapping，而<span class="math inline">\(G_d\)</span>负责进行整体的、粗糙的。判别器没给图，结构和之前一样，只不过<span class="math inline">\(D_d\)</span>负责判断全局的真假，而<span class="math inline">\(D_o\)</span>负责局部细节的。同样的<span class="math inline">\(D_d\)</span>的输入是经过两次下采样的图片。</p>
<h2 id="基于gan的半监督学习方法">基于GAN的半监督学习方法</h2>
<p>实际上，先用一些传统TMO得到LDR，再选出指标最好的一个，也不能避免传统TMO的伪影问题。按理来说，使用摄影师手动调整过的图片是最好的，但是这样又太费时。</p>
<p><a class="link" href="https://ieeexplore.ieee.org/abstract/document/9454333"  target="_blank" rel="noopener"
    >A Real-Time Semi-Supervised Deep Tone Mapping Network</a>提出了一种半监督方法，文中指出了直接使用CycleGAN这样的无监督方法效果不好，需要加一点配对数据来提升能力。本文的网络结构如下</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 155; 
			flex-basis: 373px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/11.jpg" data-size="609x391">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/11.jpg"
			width="609"
			height="391"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/11_hu497f12bf92888b76035358130e6c4554_42085_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/11_hu497f12bf92888b76035358130e6c4554_42085_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="11.jpg">
	</a>
	
	<figcaption>11.jpg</figcaption>
	
</figure></p>
<p>作者首先指出，RGB转化为HSV，并且分别处理亮度和饱和度是最好的。本文引入配对标注数据的方法是，在原有的CycleGAN的基础上加上一个配对数据的<span class="math inline">\(L_1\)</span>损失，对于<span class="math inline">\(V\)</span>通道</p>
<p><span class="math display">\[L_1 = E_{(h,l_p)}||G_{H2L}(h)-l_p||_1
\]</span></p>
<p>其中<span class="math inline">\(h\)</span>指HDR图，<span class="math inline">\(l_p\)</span>指其对应的人类专家编辑的LDR图标注。而对于<span class="math inline">\(S\)</span>通道</p>
<p><span class="math display">\[L_1 = E_{(a,b_p)}||G_{A2B}(a)-b_p||_1
\]</span></p>
<p>形式一样。然后将这两个损失分别加到各自的CycleGAN之前有的那一堆损失里。遇到没有配对图片的情况，损失就取<span class="math inline">\(0\)</span>。</p>
<p>关于生成器和判别器的网络架构，比较公式就略过了，重要的思想是前面的添加配对数据的部分。</p>
<h2 id="基于dnn的无监督学习方法">基于DNN的无监督学习方法</h2>
<p><a class="link" href="https://openaccess.thecvf.com/content/ICCV2021/html/Vinker_Unpaired_Learning_for_High_Dynamic_Range_Image_Tone_Mapping_ICCV_2021_paper.html"  target="_blank" rel="noopener"
    >Unpaired learning for high dynamic range image tone mapping</a>一文中介绍了一种无监督的方法，即不使用配对的HDR-LDR数据。网络结构如下</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 330; 
			flex-basis: 793px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/12.jpg" data-size="1048x317">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/12.jpg"
			width="1048"
			height="317"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/12_huf21ad4107f9d175c0a4432b03c8d31f7_37861_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/12_huf21ad4107f9d175c0a4432b03c8d31f7_37861_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="12.jpg">
	</a>
	
	<figcaption>12.jpg</figcaption>
	
</figure></p>
<p>和大多数工作一样，本文仅在亮度上进行Tone mapping，之后再进行颜色恢复，特别的，使用RGB转YUV后的Y通道。本文并算不上GAN，虽然他确实有判别器，但是他并没有生成器。首先本文将<span class="math inline">\(Y\)</span>通道应用一个log型的曲线进行动态范围压缩</p>
<p><span class="math display">\[Y_c(x)=\log\left(\lambda\dfrac{Y(x)}{\max(Y)}+\varepsilon\right)/\log(\lambda+\varepsilon)
\]</span></p>
<p>其中<span class="math inline">\(\lambda\)</span>是一个可调参数，而<span class="math inline">\(\varepsilon=0.05\)</span>是一个常数。本文的网络<span class="math inline">\(N\)</span>的作用是在<span class="math inline">\(Y_c\)</span>上进行微调，以保证局部对比度，移除其他伪影。为了保证<span class="math inline">\(N\)</span>输出的亮度特征和LDR的特征相似，本文引入了判别器，判断一个输入图片是否为LDR，另外，本文学习前人的工作使用了多尺度的判别器。即将真假的LDR的原图、下采样过1次、2次的图分别用三个判别器进行判别。判别器的误差如下</p>
<p><span class="math display">\[L_D = \sum_{k=\{0,1,2\}}\left(E_{Y_L\sim LDR}[D_k(\downarrow^kY_L)-1]^2+E_{Y\sim HDR}[D_k(\downarrow^kN(Y_c))]^2\right)
\]</span></p>
<p>其中<span class="math inline">\(\downarrow^k\)</span>指的是下采样过<span class="math inline">\(k\)</span>次。这个判别器会给网络<span class="math inline">\(N\)</span>传递一个误差，目的是让其亮度表现的像LDR</p>
<p><span class="math display">\[L_{natural} = \sum_{k=\{0,1,2\}}E_{Y\sim HDR}[D_k(\downarrow^kN(Y_c))-1]^2
\]</span></p>
<p>以上的部分只是让生成的LDR亮度看起来就像人类专家精心编辑过的LDR图片一样，但是他并没有要求假LDR图片拥有和原图一样的“结构”，也无法解决局部对比度等问题，所以：</p>
<p><span class="math display">\[L_{struct} = \sum_{k=\{0,1,2\}}\rho(\downarrow^kY_c,\downarrow^kN(Y_c))
\]</span></p>
<p>其中</p>
<p><span class="math display">\[\rho(I,J) = \dfrac{1}{n_p}\sum_{p_I, p_J}\dfrac{cov(p_I, p_J)}{\sigma(p_I)\sigma(p_J)}
\]</span></p>
<p>是皮尔逊相关系数，其中<span class="math inline">\(p_I, p_J\)</span>是两张图片上对应的<span class="math inline">\(5\times 5\)</span>的小块。</p>
<p>最后是关于前面说的log型曲线的参数选择，使用数值算法求解最优值</p>
<p><span class="math display">\[\lambda = \text{argmin}-\sum_lH_l(Y_c)\log(H_l(LDR))
\]</span></p>
<p>其中<span class="math inline">\(H(Y_c)\)</span>是<span class="math inline">\(Y_c\)</span>的直方图，而<span class="math inline">\(H(LDR)\)</span>则是数百张精挑细选过的LDR图片的直方图均值。直方图总共有<span class="math inline">\(20\)</span>个bins。作者说只考虑HDR图片的亮度最大值最小值可能会受到噪声的干扰，效果不好。</p>
<h2 id="基于扩散模型的无监督学习方法">基于扩散模型的无监督学习方法</h2>
<p>如果说上面那个还要用HDR图片来训练的话，下面这个方法连HDR都不用了，干脆就是只用LDR训练，直接在HDR上推理。</p>
<p><a class="link" href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhu_Zero-Shot_Structure-Preserving_Diffusion_Model_for_High_Dynamic_Range_Tone_Mapping_CVPR_2024_paper.html"  target="_blank" rel="noopener"
    >Zero-Shot Structure-Preserving Diffusion Model for High Dynamic Range Tone Mapping</a>发现HDR和其LDR的结构是“几乎相同”的，细微的差别实际上是不完美的TMO造成的。本文使用的结构指标如下</p>
<p><span class="math display">\[\mu(i,j) = \sum^K_{x=-K}\sum^K_{y=-K} \omega(x,y)I(i+x, j+y)
\]</span></p>
<p><span class="math display">\[\sigma(i,j) = \sqrt{\sum^K_{x=-K}\sum^K_{y=-K} \omega(x,y)[I(i+x, j+y)-\mu(i,j)]^2}
\]</span></p>
<p><span class="math display">\[\hat I(i,j) = \dfrac{I(i,j)-\mu(i,j)}{\sigma(i,j)+\varepsilon}
\]</span></p>
<p>其中<span class="math inline">\(\omega(x, y)\)</span>是高斯核。最后这个公式就是结构指标，作者称之为MSCN，作者通过实验验证，在JS散度的意义上，LDR和HDR图片的结构分布是极为相似的，或者直接假设为同分布。于是，本文就通过这个结构上同分布的假设，在LDR和HDR上建立了知识迁移的桥梁。</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 215; 
			flex-basis: 516px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/13.jpg" data-size="843x392">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/13.jpg"
			width="843"
			height="392"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/13_hub62513831fdd55063e341cd65a6cfd62_35650_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/13_hub62513831fdd55063e341cd65a6cfd62_35650_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="13.jpg">
	</a>
	
	<figcaption>13.jpg</figcaption>
	
</figure></p>
<p>网络结构如下</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 351; 
			flex-basis: 842px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/14.jpg" data-size="1341x382">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/14.jpg"
			width="1341"
			height="382"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/14_hu97d30771154e1e74784cd0ac08d3c61c_63508_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/14_hu97d30771154e1e74784cd0ac08d3c61c_63508_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="14.jpg">
	</a>
	
	<figcaption>14.jpg</figcaption>
	
</figure></p>
<p>大致的思想就是在ControlNet上进行，输入的控制条件是MSCN、以及经过高斯模糊后的亮度图。在训练过程中，使用LDR图片，提取到其MSCN，然后对其转为YUV通道后的Y进行高斯模糊，作为亮度图。输入到Control模块后，期望其能够复现出原始的LDR图片的亮度图。而在推理过程中，使用HDR图片，也是提取到其MSCN，然后像上一篇文章一样，用log型曲线先把YUV的Y压缩一下，再进行高斯模糊。然后也是把这两个图输入到Control模块，就得到了LDR图片的亮度图，后面再进行颜色恢复。</p>
<p>文中还提出了一个SRO的操作，在每一步diffusion之后，将其中间结果也分成Y通道和MSCN，然后将MSCN偷换成原图中提取的MSCN，然后将其和Y通道合并后得到一个结果，将这个结果发送给下一步diffusion的输入。</p>
<h2 id="基于语义分割的方法">基于语义分割的方法</h2>
<p>有一些方法试图对不同的区域应用不同的Tone mapping，因为关注到显然现实生活中不同材质的物体平均亮度是不同的，可以通过这种方式来将各种物体映射到各自偏好的亮度。</p>
<p>文章<a class="link" href="https://ieeexplore.ieee.org/abstract/document/9106057/"  target="_blank" rel="noopener"
    >Tone Mapping Operators: Progressing Towards Semantic-Awareness</a>的结构如下</p>
<p><figure 
	
		class="gallery-image" 
		style="
			flex-grow: 61; 
			flex-basis: 146px"
	>
	<a href="/p/tone-mapping%E7%AE%80%E8%BF%B0/15.jpg" data-size="503x823">
		<img src="/p/tone-mapping%E7%AE%80%E8%BF%B0/15.jpg"
			width="503"
			height="823"
			srcset="/p/tone-mapping%E7%AE%80%E8%BF%B0/15_hua45c9aafae8837dad693037d13573ac2_53404_480x0_resize_q75_box.jpg 480w, /p/tone-mapping%E7%AE%80%E8%BF%B0/15_hua45c9aafae8837dad693037d13573ac2_53404_1024x0_resize_q75_box.jpg 1024w"
			loading="lazy"
			alt="15.jpg">
	</a>
	
	<figcaption>15.jpg</figcaption>
	
</figure></p>
<p>本文并没有提出什么新的网络架构。Mask的提取部分，使用了预训练的FastFCN，将其中的150个类又归为9类，然后使用形态学方法生成trimap，然后使用Alpha matting获取到精细的mask。然后，作者使用这样的方法对几百张LDR图片生成了mask，统计个各类的亮度直方图，也即学习到了个各类的目标亮度。</p>
<p>在Tone mapping的部分，作者在HDR上获取到mask后，就简单的将目前的亮度替换为目标亮度，然后再用颜色恢复方法恢复出来LDR图像。</p>
<h1 id="hdr10hdr10hlgvividaces资料解读">HDR10、HDR10+、HLG、Vivid、ACES资料解读</h1>
<p>TODO
<a class="link" href="https://en.wikipedia.org/wiki/Academy_Color_Encoding_System"  target="_blank" rel="noopener"
    >https://en.wikipedia.org/wiki/Academy_Color_Encoding_System</a></p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/%E5%A4%A7%E5%AD%A6/">大学</a>
        
            <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>
        
            <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a>
        
            <a href="/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/">图形学</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css"integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js"integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8"crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js"integrity="sha384-vZTG03m&#43;2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl"crossorigin="anonymous"
                defer="true"
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
        renderMathInElement(document.querySelector(`.article-content`), {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ]
        });})
</script>
    
</article>

    <aside class="related-contents--wrapper">
    
    
        <h2 class="section-title">相关文章</h2>
        <div class="related-contents">
            <div class="flex article-list--tile">
                
                    
<article class="has-image">
    <a href="/p/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84tone-mapping%E7%AE%80%E8%BF%B0/">
        
        
            <div class="article-image">
                <img src="/p/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84tone-mapping%E7%AE%80%E8%BF%B0/cover.0d1f710c6f0ca8af1c1e9fb6dd83630f_hud564e3059b92e0982aa63ebb5c38fa59_49577_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="" 
                        data-hash="md5-DR9xDG8MqK8cHp&#43;23YNjDw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">基于图像分割的Tone Mapping简述</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/hdr%E8%89%B2%E8%B0%83%E6%98%A0%E5%B0%84%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E7%AE%80%E8%BF%B0/">
        
        
            <div class="article-image">
                <img src="/p/hdr%E8%89%B2%E8%B0%83%E6%98%A0%E5%B0%84%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7%E7%AE%80%E8%BF%B0/cover.180758a5ee73b6597684d5908958cd17_hubce6d89c2fc230acc2296e007ead6ecd_28081_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="" 
                        data-hash="md5-GAdYpe5ztll2hNWQiVjNFw==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">HDR色调映射图像质量评价简述</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/grabcut-interactive-foreground-extraction-using-iterated-graph-cuts%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%8E%E5%A4%8D%E7%8E%B0/">
        
        
            <div class="article-image">
                <img src="/p/grabcut-interactive-foreground-extraction-using-iterated-graph-cuts%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%8E%E5%A4%8D%E7%8E%B0/cover.719f06f8145a0ec2fc378d5f11233592_huf62d2c95a51769be2a7aa0d894297a77_82581_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="" 
                        data-hash="md5-cZ8G&#43;BRaDsL8N41fESM1kg==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">GrabCut— Interactive Foreground Extraction Using Iterated Graph Cuts论文精读与复现</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/deep-bilateral-learning-for-real-time-image-enhancement%E8%AE%BA%E6%96%87%E9%80%9F%E8%AF%BB/">
        
        
            <div class="article-image">
                <img src="/p/deep-bilateral-learning-for-real-time-image-enhancement%E8%AE%BA%E6%96%87%E9%80%9F%E8%AF%BB/cover.c544671fcae18b2974e28dcd19d110e3_hu21346ed4a71566f3cd713a52c023a686_153701_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="" 
                        data-hash="md5-xURnH8rhiyl04o3NGdEQ4w==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">Deep Bilateral Learning for Real Time Image Enhancement论文速读</h2>
        </div>
    </a>
</article>
                
                    
<article class="has-image">
    <a href="/p/appprop-all-pairs-appearance-space-edit-propagation%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%8E%E5%A4%8D%E7%8E%B0/">
        
        
            <div class="article-image">
                <img src="/p/appprop-all-pairs-appearance-space-edit-propagation%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E4%B8%8E%E5%A4%8D%E7%8E%B0/cover.a6dc0aea3fa6a380f9f56b9bac533904_hueda4c761f5117f950ea670cd72a10b0b_71812_250x150_fill_q75_box_smart1.jpg" 
                        width="250" 
                        height="150" 
                        loading="lazy" 
                        data-key="" 
                        data-hash="md5-ptwK6j&#43;mo4D59WubrFM5BA==">
                
            </div>
        

        <div class="article-details">
            <h2 class="article-title">AppProp: All-Pairs Appearance-Space Edit Propagation论文精读与复现</h2>
        </div>
    </a>
</article>
                
            </div>
        </div>
    
</aside>

     
    
        
    <script src="https://utteranc.es/client.js" 
        repo="kegalas/blogComments"
        issue-term="pathname"
        
        crossorigin="anonymous"
        async
        >
</script>

<style>
    .utterances {
        max-width: unset;
    }
</style>

<script>
    function setUtterancesTheme(theme) {
        let utterances = document.querySelector('.utterances iframe');
        if (utterances) {
            utterances.contentWindow.postMessage(
                {
                    type: 'set-theme',
                    theme: `github-${theme}`
                },
                'https://utteranc.es'
            );
        }
    }

    addEventListener('message', event => {
        if (event.origin !== 'https://utteranc.es') return;
        setUtterancesTheme(document.documentElement.dataset.scheme)
    });

    window.addEventListener('onColorSchemeChange', (e) => {
        setUtterancesTheme(e.detail)
    })
</script>


    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 KegalaS的个人博客
    </section>
    
    <section class="powerby">
         <br />
        
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">目录</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ol>
    <li><a href="#如何显示hdr">如何显示HDR</a></li>
    <li><a href="#tone-mapping的作用">Tone Mapping的作用</a></li>
    <li><a href="#只考虑单个像素亮度的tmo">只考虑单个像素亮度的TMO</a></li>
    <li><a href="#考虑周围像素亮度值的tmo">考虑周围像素亮度值的TMO</a></li>
    <li><a href="#考虑像素颜色值的tmo">考虑像素颜色值的TMO</a></li>
    <li><a href="#深度学习的tmo">深度学习的TMO</a>
      <ol>
        <li><a href="#基于gan的监督学习方法">基于GAN的监督学习方法</a></li>
        <li><a href="#基于gan的半监督学习方法">基于GAN的半监督学习方法</a></li>
        <li><a href="#基于dnn的无监督学习方法">基于DNN的无监督学习方法</a></li>
        <li><a href="#基于扩散模型的无监督学习方法">基于扩散模型的无监督学习方法</a></li>
        <li><a href="#基于语义分割的方法">基于语义分割的方法</a></li>
      </ol>
    </li>
    <li><a href="#hdr10hdr10hlgvividaces资料解读">HDR10、HDR10+、HLG、Vivid、ACES资料解读</a></li>
  </ol>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
